import io
import json

import itertools
import numpy as np

from pathlib import Path

from ...internal import localfs
from ... import scan
from .. import upload

# output generated by the scan program for the small.sgy file
small_manifest = '''
{
    "byteorder": "big",
    "format": 1,
    "samples": 50,
    "sampleinterval": 4000,
    "byteoffset-first-trace": 3600,
    "sample-value-min" : 1.2100000381469727,
    "sample-value-max" : 5.240489959716797,
    "dimensions": [
        [1, 2, 3, 4, 5],
        [20, 21, 22, 23, 24],
        [     0,    4000,   8000,  12000,  16000,  20000,  24000,  28000,
          32000,   36000,  40000,  44000,  48000,  52000,  56000,  60000,
          64000,   68000,  72000,  76000,  80000,  84000,  88000,  92000,
          96000,  100000, 104000, 108000, 112000, 116000, 120000, 124000,
          128000, 132000, 136000, 140000, 144000, 148000, 152000, 156000,
          160000, 164000, 168000, 172000, 176000, 180000, 184000, 188000,
          192000, 196000]
    ],
    "key1-last-trace": {
        "1": 4,
        "2": 9,
        "3": 14,
        "4": 19,
        "5": 24
    },
    "key-words": [189, 193],
    "guid": "86f5f8f783fabe2773531d5529226d37b6c9bdcf"
}
'''
source = Path(__file__).resolve().parent / 'small.sgy'

def test_upload_manifest_all_keys(tmp_path):
    filesys = localfs(tmp_path)
    fragment_shape = (4, 4, 4)
    meta = json.loads(small_manifest)
    guid = meta['guid']

    with open(source, 'rb') as src:
        upload(meta, fragment_shape, src, source, filesys)

    with open(tmp_path / Path(f'{guid}/manifest.json')) as f:
        manifest = json.load(f)

    assert manifest['format-version']   == 1
    assert manifest['upload-filename']  == 'small.sgy'
    assert manifest['guid']             == guid
    assert manifest['line-numbers']     == meta['dimensions']
    assert manifest['line-labels']      == ['inline', 'crossline', 'time']
    assert manifest['sample-value-min'] == 1.2100000381469727
    assert manifest['sample-value-max'] == 5.240489959716797

def test_upload_proper_volume_expected_fragment_ids(tmp_path):
    filesys = localfs(tmp_path)
    fragment_shape = (4, 4, 4)
    meta = json.loads(small_manifest)
    with open(source, 'rb') as src:
        upload(meta, fragment_shape, src, source, filesys)

    expected = sorted([
        f'{i}-{j}-{k}.f32'
        for i in range(2)
        for j in range(2)
        for k in range(13)
    ])

    guid = meta['guid']
    root = tmp_path / Path(f'{guid}/src/4-4-4')
    uploaded = sorted([p.name for p in root.iterdir()])
    assert uploaded == expected

def test_upload_missing_leading_traces(tmp_path):
    # copy small, but remove the first 3 traces
    cropped = tmp_path / Path('small.sgy')
    with open(source, mode = 'rb') as src, open(cropped, mode = 'wb') as dst:
        chunk = src.read(3600) # text and binary header
        dst.write(chunk)
        tracesize = 240 + 50 * 4
        src.seek(tracesize * 3, io.SEEK_CUR)
        chunk = src.read()
        dst.write(chunk)

    filesys = localfs(tmp_path)
    fragment_shape = (4, 4, 4)

    # shift the traceno of last-seen-in-line by the traces removed
    meta = json.loads(small_manifest)
    meta['key1-last-trace'] = {
        key: trno - 3 for key, trno in meta['key1-last-trace'].items()
    }
    with open(cropped, 'rb') as src:
        upload(meta, fragment_shape, src, cropped, filesys)

    expected = sorted([
        f'{i}-{j}-{k}.f32'
        for i in range(2)
        for j in range(2)
        for k in range(13)
    ])

    guid = meta['guid']
    root = tmp_path / Path(f'{guid}/src/4-4-4')
    uploaded = sorted([p.name for p in root.iterdir()])
    assert uploaded == expected

def test_upload_missing_trailing_traces(tmp_path):
    # copy small, but remove the last 3 traces
    cropped = tmp_path / Path('small.sgy')
    with open(source, mode = 'rb') as src, open(cropped, mode = 'wb') as dst:
        chunk = src.read(3600) # text and binary header
        dst.write(chunk)
        tracesize = 240 + 50 * 4
        chunk = src.read((25 - 3) * tracesize)
        dst.write(chunk)

    filesys = localfs(tmp_path)
    fragment_shape = (4, 4, 4)

    meta = json.loads(small_manifest)
    meta['key1-last-trace']['5'] = 21
    with open(cropped, 'rb') as src:
        upload(meta, fragment_shape, src, cropped, filesys)

    expected = sorted([
        f'{i}-{j}-{k}.f32'
        for i in range(2)
        for j in range(2)
        for k in range(13)
    ])

    guid = meta['guid']
    root = tmp_path / Path(f'{guid}/src/4-4-4')
    uploaded = sorted([p.name for p in root.iterdir()])
    assert uploaded == expected

def test_upload_missing_interspersed_traces(tmp_path):
    # copy small, but remove the last 3 traces
    cropped = tmp_path / Path('small.sgy')
    with open(source, mode = 'rb') as src, open(cropped, mode = 'wb') as dst:
        chunk = src.read(3600) # text and binary header
        dst.write(chunk)
        tracesize = 240 + 50 * 4

        # remove one trace from each line along the "diagonal", i.e.  skip the
        # first trace on the first line, the second trace on the second line
        # etc.
        meta = json.loads(small_manifest)
        for i in range(5):
            chunk = src.read(i * tracesize)
            dst.write(chunk)
            src.seek(tracesize, io.SEEK_CUR)
            chunk = src.read((4 - i) * tracesize)
            dst.write(chunk)
            meta['key1-last-trace'][str(i+1)] -= i + 1

    filesys = localfs(tmp_path)
    fragment_shape = (4, 4, 4)

    with open(cropped, 'rb') as src:
        upload(meta, fragment_shape, src, cropped, filesys)

    expected = sorted([
        f'{i}-{j}-{k}.f32'
        for i in range(2)
        for j in range(2)
        for k in range(13)
    ])

    guid = meta['guid']
    root = tmp_path / Path(f'{guid}/src/4-4-4')
    uploaded = sorted([p.name for p in root.iterdir()])
    assert uploaded == expected

def test_upload_scrambled_traces(tmp_path):
    """
    This is actually the stronger test, as it checks that the uploaded
    fragments are bitwise identical, desipte the source volumes being
    differently sorted.
    """
    # scramble the input file by randomly reorganizing the traces
    scrambled = tmp_path / Path('small.sgy')
    with open(source, mode = 'rb') as src, open(scrambled, mode = 'wb') as dst:
        chunk = src.read(3600) # text and binary header
        dst.write(chunk)
        dtype = np.dtype([
            ('header', 'b', 240),
            ('trace', 'f4', 50),
        ])
        small = np.ones(5 * 5, dtype = dtype)
        src.readinto(small)
        np.random.shuffle(small)
        dst.write(small)

    meta = json.loads(small_manifest)
    fragment_shape = (4, 4, 4)
    with open(source, 'rb') as src:
        fs = localfs(tmp_path)
        fs.mkdir('sorted')
        fs.cd('sorted')
        upload(meta, fragment_shape, src, source, fs)

    # the scrambled file must be re-scanned, because the new key1-last-trace is
    # non-deterministic
    from ...scan.__main__ import main as scan_main
    m = json.loads(scan_main([str(scrambled)]))

    filesys = localfs(tmp_path)
    with open(scrambled, 'rb') as src:
        upload(m, fragment_shape, src, scrambled, filesys)

    guid = meta['guid']
    sorted_root   = tmp_path / Path(f'sorted/{meta["guid"]}/src/4-4-4')
    uploaded_root = tmp_path / Path(f'{m["guid"]}/src/4-4-4')

    files = zip(
        sorted(uploaded_root.iterdir()),
        sorted(sorted_root.iterdir()),
    )
    for up, orig in files:
        assert up.name == orig.name
        with open(up, 'rb') as f, open(orig, 'rb') as g:
            from_scramble = f.read()
            from_orig     = g.read()
        assert from_scramble == from_orig

def test_upload_regression(tmp_path):
    """
    Verify fragments generated by upload against a previously verified version.
    """
    meta = json.loads(small_manifest)
    fragment_shape = (3, 3, 28)

    with open(source, mode='rb') as src:
        fs = localfs(tmp_path)
        fs.mkdir('regression')
        fs.cd('regression')

        upload(meta, fragment_shape, src, 'small.sgy', fs)

    refdir = Path(__file__).resolve().parent / 'small'
    resdir = tmp_path / f'regression/{meta["guid"]}/src/3-3-28/'

    product = itertools.product([0, 1], repeat=3)
    fragments = ['{}-{}-{}.f32'.format(x, y, z) for x, y, z in product]

    for fragment in fragments:
        ref = refdir / fragment
        res = resdir / fragment
        with open(res, 'rb') as result, open(ref, 'rb') as reference:
            result = np.fromfile(result, 'f4')
            reference = np.fromfile(reference, 'f4')
            np.testing.assert_allclose(result, reference, rtol=1e-10)
